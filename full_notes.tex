\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage[noend]{algpseudocode}

\title{Advanced Algorithms and Datastructures - Full Notes}
\author{André O. Andersen}
\date{2021}

\begin{document}

\maketitle

\section*{Max Flow}
\subsection*{General Knowledge}
In the maximum-flow problem, we wish to compute the greatest rate at which we can ship material from the source to the sink without violating any capacity constraints. 
\\
\\
A \textit{\textbf{flow network}} $G = (V, E)$ is a directed graph in which each edge $(u, v) \in E$ has a nonnegative \textit{\textbf{capacity}} $c(u, v) \geq 0$. We further require that if $E$ contains an edge $(u, v)$, then here is no edge $(v, u)$ in the reverse direction. If $(u, v) \notin E$, then we define $c(u, v) = 0$, and we disallow self-loops. We distinguish two vertices in a flow network a \textit{\textbf{source}} $s$ and a \textit{\textbf{sink}} $t$. For convenience, we assume that each vertex lies on some path from the source to the sink.
\\
\\
A \textit{\textbf{flow}} in $G$ is a real-valued function $f: V \times V \rightarrow \mathbb{R}$ taht satisfies the following two properties:
\begin{enumerate}
    \item \textbf{Capacity constraint}: For all $u, v \in V$, we require $0 \leq f(u, v) \leq c(u, v)$
    \item For all $u \in V - \{s, t\}$, we require
    $$\sum_{v \in V} f(v, u) = \sum_{v \in V} f(u, v).$$
    When $(u, v) \notin E$ there can be no flow from $u$ to $v$, hence $f(u, v) = 0$
\end{enumerate}
We call the nonnegative quantity $f(u, v)$ the flow from vertex $u$ to vertex $v$. The \textit{\textbf{value}} $|f|$ of a flow $f$ is defined as
$$|f| = \sum_{v \in V} f(s, v) - \sum_{v \in V} f(v, s).$$
In the \textit{\textbf{maximum-flow problem}}, we are given a flow network $G$ with source $s$ and $t$, and we wish to maximize $|f|$ such the capacity constraint and the flow conservation are not violated.
\\
\\
We call the two edges $(v_1, v_2)$ and $(v_2, v_1)$ \textit{\textbf{antiparallel}}. If we wish to model a flow problem with antiparallel adges, we must transform the network into an equivalent one containing no antiparallel edges. This is done by choosing one of the two antiparallel edges, in this case $(v_1, v_2)$, and split it by adding a new vertex $v'$ and replacing edge $(v_1, v_2)$ with the pair of edges $(v_1, v')$ and $(v', v_2)$. We also set the capacity of both new edges to the capacity of the original edge. The resulting network satisfies the property that if an edge is in the network, the reverse edge is not.
\\
\\
A maximum-flow problem may have several sources and sinks, rather than just one of each. Fortunately, this problem is no harder than ordinary maximum flow. We can reduce the problem of determining a maximum flow in a network with multiple sources and multiple sinks to an ordinary maximum-flow problem. We add a \textit{\textbf{supersource}} $s$ and add a directed edge $(s, s_i)$ with capacity $c(s, s_i) = \infty$ for each $i = 1, 2, ..., m$. We also create a new \textit{\textbf{supersink}} $t$ and add a directed edge $(t_i, t)$ with capacity $c(t_i, t) = \infty$ for each $i = 1, 2, ..., n$, where $m$ and $n$ are the amount of sources and sinks in the original graph, respectively.
\\
\\
The Ford-Fulkerson method is a method for solving the maximum-flow problem. It iteratively increases the value of the flow. We start with $f(u, v) = 0$ for all $u, v \in V$, giving an initial flow of value $0$. At each iteration, we increase the flow value in $g$ by finding and "augmenting path" (a path from the source to sink in the residual network) in an associated "residual network" $G_f$. Once we know the edges of an augmenting path in $G_f$, we can easily identify specific edges in $G$ for which we can change the flow so that we increase the value of the flow. We repeatedly change the flow until the residual network has no more augmenting paths. The max-flow min-cut theorem will show that upon termination, this process yields a maximum flow.
\\
\\
The residual network $G_f$ consists of edges with capacities that represent how we can change the flow on edges of $G$. An edge of the flow network can admit an amount of additional flow equal to the edge's capacity minus the flow on that edge. If that value if positive, we place that edge into $G_f$ with a "residual capacity" of $c_f(u, v) = c(u, v) - f(u, v)$. The only edge in $G$ that are in $G_f$ are those that can admit more flow.
\\
\\
The residual network $G_f$ may also contain edges that are not in $G$.  In order to represent a possible decrease of a positive flow $f(u, v)$ on an edge in $G$, we place an edge $(v, u)$ into $G_f$ with residual capacity $c_f(v, u) = f(u, v)$ - that is, an edge that can admit flow in the opposite direction to $(u, v)$, at most conaceling out the flow on $(u, v)$.
\\
\\
The \textit{\textbf{residual capaicty}} $c_f(u, v)$ is thus
\begin{center}
    \begin{equation}
        \label{eqn:26.2}
        c_f(u, v) =
        \begin{cases}
            c(u, v) - f(u, v) & \text{if } (u, v) \in E \\
            f(v, u) & \text{if } (v, u) \in E \\
            0 & \text{otherwise}.
        \end{cases}
    \end{equation}
\end{center}
Because of our assumption that $(u, v) \in E$ implies $(v, u) \notin E$, exactly one case in equation (\ref{eqn:26.2}) applies to each ordered pair of vertices.
\\
\\
Given a flow network $G = (V, E)$ and a flow $f$, the \textit{\textbf{residual network}} of $G$ induced by $f$ is $G_f = (V, E_f)$, where
$$
E_f = \{(u, v) \in V \times V : c_f (u, v) > 0\}.
$$
That is, each edge of the residual network, or \textit{\textbf{residual edge}}, can admit a flow that is greater than $0$. The edge in $E_f$ are either edges in $E$ or their reversals, and thus $|E_f| \leq 2 |E|$.
\\
\\
A flow in a residual network pro vides a roadmap for adding flow to the original flow network. If $f$ is a flow in $G$ and $f'$ is a flow in the corresponding residual network $G_f$, we define $f \uparrow f'$, the \textit{\textbf{augmentation}} (or "change") of flow $f$ by $f'$, to be a functio nfrom $V \times V$ to $\mathbb{R}$, defined by
\begin{center}
    \begin{equation}
        \label{eqn:26.4}
        (f \uparrow f')(u, v) =
        \begin{cases}
            f(u, v) + f'(u, v) - f'(v, u) & \textit{if } (u, v) \in E \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation}
\end{center}
An \textit{\textbf{augmenting path}} $p$ is a simple path from $s$ to $t$ in the residual network $G_f$. We call the maximum amount by which we can increase the flow on e ach edge in an augmenting path $p$ the \textit{\textbf{residual capacity}} of $p$, given by
$$
c_f(p) = \min\{c_f(u, v) : (u, v) \text{is on } p\}.
$$

\noindent \textbf{Lemma 26.2} \\
Let $G = (V, E)$ be a flow network, let $f$ be a flow in $G$ and let $p$ be an augmenting path in $G_f$. Define a function $f_p : V \times V \rightarrow \mathbb{R}$ by
\begin{center}
    \begin{equation}
        \label{eqn:26.8}
        f_p(u, v) =
        \begin{cases}
            c_f(p) & \text{if } (u, v) \text{ is on } p \\
            0 & \textit{otherwise}.
        \end{cases}
    \end{equation}
\end{center}
Then, $f_p$ is a flow in $G_f$ with value $|f_p| = c_f(p) > 0$.
\\
\\
A \textit{\textbf{cut}} $(S, T)$ of flow network $G = (V, E)$ is a partition of $V$ into $S$ and $T = V - S$ such that $s \in S$ and $t \in T$. If $f$ is a flow, then the \textit{\textbf{net flow}} $f(S, T)$ across the cut $(S, T)$ is defiend to be
\begin{center}
    \begin{equation}
        f(S, T) = \sum_{u \in S} \sum_{v \in T} f(u, v) - \sum_{u \in S} \sum_{v \in T} f(v, u).
    \end{equation}
\end{center}
The \textit{\textbf{capacity}} of the cut $(S, T)$ is
$$
c(S, T) = \sum_{u \in S} \sum_{v \in T} c(u, v).
$$
A \textit{\textbf{minimum cut}} of a network is a cut whose capacity is minimum over all cuts of the network.
\\
\\
The running time of Ford-Fulkerson depends on how we find the augmenting path $p$. If we choose it poorly, the algorithm might not even terminate - the ford-fulkerson method might fail to terminate only if edge capacities are irriational numbers. If we assume that the capacities are integers, then Ford-Fulkerson has a running time of $O(E \cdot |f^*|)$, where $f^*$ is a max flow. This is because the capacities are integers, we know Ford-Fulkerson increases the value by atleast $1$ at each step, making the runtime be bounded by the max flow ($|f^*|$) and $E$ is used for finding an augmenting path.
\\
\\
We can improve the bound on Ford-Fulkerson by finding the augmenting path $p$ with a breadth-first search. That is, we choose the augmenting path as a \textit{shortest} path from $s$ to $t$ in the residual networ, where each edge has unit distance (weight). This is called the \textit{\textbf{Edmonds-Karp Algorithm}} and it runs in $O(VE^2)$.
\\
\\
Because we can implement each iteration of Ford-Fulkerson in $O(E)$ time when we find the augmenting path by breadth-first search and the total number of flow augmentations is $O(VE)$, the total running time of the Edmonds-Karp algorithm is $O(VE^2)$. 

\clearpage

\subsection*{Examples}
\subsubsection*{Maximum bipartite matching}
Given an undirected graph $G = (V, E)$, a \textit{\textbf{matching}} is a subset of edges $M \subseteq E$ such taht for all vertices $v \in V$, at most one edge of $M$ is incident on $v$. We say that a vertex $v \in V$ is \textit{\textbf{matched}} by the matching $M$ if some edge in $M$ is incident on $v$; otherwise, $v$ is \textit{\textbf{unmatched}}. A \textit{\textbf{maximum matching}} is amatching of maximum cardinality. In this section, we shall restrict our attention to findin maximum matchings in bipartite graphs: graphs in which the vertex set can be partitioned into $V = L \cup R$, where $L$ and $R$ are disjoint and all edges in $E$ go between $L$ and $R$. We furhter assume that every vertex in $V$ has at least one incident edge.
\\
\\
We can use the Ford-Fulkerso nmethod to find a maximum matching in an undirected bipartite graph $G = (V, E)$ in time polynomial in $|V|$ and $|E|$. The trick is to construct a flow network in which flows correspond to matchings. We define the \textit{\textbf{corresponding flow network}} $G' = (V', E')$ for the bipartite graph $G$ as follows. We let the source $s$ and sink $t$ be new vertices not in $V$, $V' = V \cup \{s, t\}$, and 
$$
E' = \{(s, u) : u \in L\} \cup \{(u, v) : (u, v) \in E\} \cup \{(v, t) : v \in R\}.
$$ 
To complete the construction, we assign unit capacity to each edge in $E'$. Since each vertex in $v$ has at least one incident edge, $|E| \geq |V|/2$. Thus, $|E| \leq |E'| = |E| + |V| \leq 3 |E´|$, and so $|E'| = \Theta(E)$.

\clearpage

\subsection*{Proofs}
\subsubsection*{Lemma 26.1}
Let $G = (V, E)$ be a flow network with source $s$ and sink $t$, and let $f$ be a flow in $G$. Let $G_f$ be the residual network of $G$ induced by $f$, and let $f'$ be a flow in $G_f$. Then the function $f \uparrow f'$ is a flow in $G$ with value $|f \uparrow f'| = |f| + |f'|$.
\\
\\
\textbf{\textit{Proof}} We first verify that $f \uparrow f'$ obeys the capacity constraint for each edge in $E$ and flow conservation at each vertex in $V - \{s, t\}$. \\
For the capacity constraint, first observe that if $(u, v) \in E$, then $c_f(v, u) = f(u, v)$. Therfore, we have $f'(v, u) \leq c_f(v, u) = f(u, v)$, and hence
\begin{center}
    \begin{align*}
        (f \uparrow f')(u, v) & = f(u, v) + f'(u, v) - f'(v, u) && \text{(by equation (\ref{eqn:26.4}))}\\
        & \geq f(u, v) + f'(u, v) - f(u, v) && \text{(because } f'(v, u) \leq f(u, v) \text{)}\\
        & = f'(u, v) \\
        & \geq 0.
    \end{align*}
\end{center}
In addition,
\begin{center}
    \begin{align*}
        (f \uparrow f')(u, v) &= f(u, v) + f'(u, v) - f'(v, u) && \text{(by equation (\ref{eqn:26.4})} \\
        &\leq f(u, v) + f'(u, v) && \text{(because flows are nonegative)} \\
        &\leq f(u, v) + c_f(u, v) && \text{(capacity constraint)} \\
        &= f(u, v) + c(u, v) - f(u, v) && \text{(definition of } c_f \text{)} \\
        &= c(u, v).
    \end{align*}
\end{center}
To show that flow conservation holds and that $|f \uparrow f'| = |f| + |f'|$, we first prove the claim that for all $u \in v$, we have
\begin{center}
    \begin{equation}
        \label{eqn:26.5}
        \sum_{v \in v} (f \uparrow f')(u, v) - \sum_{v \in V} (f \uparrow f')(v, u) = \sum_{v \in V} f(u, v) - \sum_{v \in V} f(v, u) + \sum_{v \in V} f'(u, v) - \sum_{v \in V} f'(v, u).
    \end{equation}
\end{center}
Because we disallow antiparallel edges in $G$, we know that for each vertex $u$, there can be an edge $(u, v)$ or $(v, u)$ in $g$, but never both. For a fixed vertex $u$, let's define $V_1(u) = \{v: (u, v) \in E\}$ and $V_2(u) = \{v: (v, u) \in E\}$. By the definition of flow augmentation in euqation (\ref{eqn:26.4}), only vertices in $V_1(u)$ can have positive $(f \uparrow f')(u,v)$, and only vertices in $V_2(u)$ can have positive $(f \uparrow f')(v, u)$. Thus, we have
\begin{center}
    \begin{align*}
        & \sum_{v \in V} (f \uparrow f')(u, v) - \sum_{v \in V} (f \uparrow f') (v, u) \\
        &= \sum_{v \in V_1(u)} (f \uparrow f') (u, v) - \sum_{v \in V_2(u)} (f \uparrow f') (v, u) \\
        &= \sum_{v \in V_1(u)} \left( f(u, v) + f'(u, v) - f'(v, u) \right) - \sum_{v \in V_2(u)} \left( f(v, u) + f'(v, u) - f'(u, v) \right) \\
        &= \sum_{v \in V_1 (u)} f(u, v) + \sum_{v \in V_1 (u)} f'(u, ) - \sum_{v \in V_1 (u)} f'(v, u) - \sum_{v \in V_2(u)} f(v, u) - \sum_{u \in V_2(u)} f'(v, u) + \sum_{v \in V_2 (u)} f'(u, v) \\
        &= \sum_{v \in V_1 (u)} f(u, v) - \sum_{v \in V_2 (u)} f(v, u) + \sum_{v \in V_1(u)} f'(u, v) + \sum_{v \in V_2 (u)} f'(u, v) - \sum_{v \in V_1 (u)} f'(v, u) - \sum_{v \in V_2 (u)} f'(v, u)
    \end{align*}
\end{center}
In the last equation we can extend all four summations to sum over $v$, since each additional term has value $0$. With all four summations over $V$, instead of just subsets of $V$, we get equation (\ref{eqn:26.5}). \\
Now we are ready to prove flow conservation for $f \uparrow f'$ and that $|f \uparrow f'| = |f| + |f'|$. For the latter property, let $u = s$ in equation (\ref{eqn:26.5}). Then, we have
\begin{center}
    \begin{align*}
        |f \uparrow f'| &= \sum_{v \in V} (f \uparrow f')(s, v) - \sum_{v \in V} (f \uparrow f') (v, s) \\
        &= \sum_{v \in V} f(s, v) - \sum_{v \in V} f(v, s) + \sum_{v \in V} f'(s, v) - \sum_{v \in V} f'(v, s) \\
        &= |f| + |f'|.
    \end{align*}
\end{center}
For flow conservation, observe that for any vertex $u$ that is neither $s$ not $t$, flow conservation for $f$ and $f'$ means that the right-hand side of equation (\ref{eqn:26.5}) is $0$, and thus $\sum_{v \in V} (f \uparrow f')(u, v) = \sum_{v \in V} (f \uparrow f') (v, u)$.

\subsubsection*{Corollary 26.3}
Let $G = (V, E)$ be a flow network, let $f$ be a flow in $G$, and let $p$ be an augmenting path in $G_f$. Let $f_p$ be defined as in equation (\ref{eqn:26.8}), and suppose that we augment $f$ by $f_p$. then the function $f \uparrow f_p$ is a flow in $G$ with value $|f \uparrow f_p| = |f| + |f_p| > |f|$. \\
\textit{\textbf{Proof}} Immediate from Lemmas 26.1 and 26.2.

\subsubsection*{Lemma 26.4}
Let $f$ be a flow in a flow network $G$ with source $s$ and sink $t$, and let $(S, T)$ be any cut of $G$. Then the net flow across $(S, T)$ is $f(S, T) = |f|$. \\
\textbf{\textit{Proof}} \\
We extend the definition of net flow from cuts to arbitrary pairs of subsets of $V$ as follows. For any $A \subseteq V$ and $B \subseteq V$,
$$
f(A, B) = \sum_{u \in A} \sum_{v \in B} f(u, v) - f(v, u).
$$
Now we proceed with the proof. The definition of net flow above immediately implies $f(S, S) = 0$, since the we are at some point adding $f(u, v)$, however, at some point $u$ and $v$ switches place, hence why we subtract $f(u, v)$ again. Thus, we have
$$f(S, T) = f(S, S) + f(S, T) = f(S, V) = \sum_{u \in S} \sum_{v \in V} (f(u, v) - f(v, u)).$$
On the right-hand side, every $u \in S \backslash \{s\}$ contributes $0$ to the sum due to flow conservation. Thus,
$$
f(S, T) = \sum_{u \in \{s\}} \sum_{v \in \{V\}} (f(u, v) - f(v, u)) = \sum_{v \in V} (f(s, v) - f(v, s)) = |f|.
$$

\subsubsection*{Corollary 26.5}
The value of any flow $f$ in a flow network $G$ is bounded from above by the capacity of any cut of $G$.
\textit{\textbf{Proof}} Let $(S, T)$ be any cut of $G$ and let $f$ be any flow. By Lemma 26.4 and the capacity constraint,
\begin{center}
    \begin{align*}
        |f| &= f(S, T) \\
        &= \sum_{u \in S} \sum_{v \in T} f(u, v) - \sum_{u \in S} \sum_{v \in T} f(v, u) \\
        &\leq \sum_{u \in S} \sum_{v \in T} f(u, v) \\
        &\leq \sum_{u \in S} \sum_{v \in T} c(u, v) \\
        = c(S, T).
    \end{align*}
\end{center}

\subsubsection*{Theorem 26.6 (Max-flow min-cut theorem)}
If $f$ is a flow in a flow network $G = (V, E)$ with source $s$ and sink $t$, then the following conditions are equivalent:
\begin{enumerate}
    \item $f$ is a maximum flow in $G$
    \item The residual network $G_f$ contains no augmenting paths
    \item $|f| = c(S, T)$ for some cut $(S, T)$ of $G$.
\end{enumerate}
\textit{\textbf{Proof}} \\
$(1) \Rightarrow (2)$: Suppose for the sake of contradiction that $f$ is a maximum flw in $G$ but that $G_f$ has an augmenting path $p$. Then, by Corollary 26.3, the flow found by augmenting $f$ by $f_p$, where $f_p$ is given by equation (\ref{eqn:26.8}), is a flow in $G$ with value strictly greater than $|f|$, contradicting the assumption that $f$ is a maximum flow. \\
$(2) \Rightarrow (3)$: Suppose that $G_f$ has no augmenting path, that is, that $G_f$ contains no path from $s$ to $t$. Define
$$
S = \{v \in V: \text{there is a path from } s \text{to } v in G_f\}
$$
and $T = V - S$. The parition $(S, T)$ is a cut: we have $s \in S$ trivially and $t \notin S$ because there is no path from $s$ to $t$ in $G_f$. Now consider a pair of vertices $u \in S$ and $v \in T$. If $(u, v) \in E$, we must have $f(u, v) = c(u, v)$, since otherwise $(u, v) \in E_f$, which would place $v$ in set $S$. If $(v, u) \in E$, we must have $f(v, u) = 0$, because otherwise $c_f(u, v) = f(v, u)$ would be positive and we we ould have $(u, v) \in E_f$, which would place $v$ in $S$. Of course, if neither $(u, v)$ nor $(v, u)$ is in $E$, htne $f(u, v) = f(v, u) = 0$. We thus have
\begin{center}
    \begin{align*}
        f(S, T) &= \sum_{u \in S} \sum_{v \in T} f(u, v) - \sum_{v \in T} \sum_{u \in S} f(v, u) \\
        &= \sum_{u \in S} \sum_{v \in T} c(u, v) - \sum_{v \in T} \sum_{u \in S} 0 \\
        &= c(S, T).
    \end{align*}
\end{center}
By Lemma 26.4 therefore $|f| = f(S, T) = c(S, T)$. \\
$(3) \Rightarrow (1)$: By Corollary 26.5, $|f| \leq c(S, T)$ for all cuts $(S, T)$. The condition $|f| = c(S, T)$ thus implies that $f$ is a maximum flow.

\subsubsection*{Lemma 6.7}
If the Edmonds-Karp algorithm is run on a flow network $G = (V, E)$ with source $s$ and sink $t$, then for all verties $v \in V - \{s, t\}$, the shortest-path distance $\delta_f(s, v)$ in the residual network $G_f$ increases monotonically with each flow augmentation. \\
\textit{\textbf{Proof}} Let $f$ be the flow just before the first augmentation that decreases some shortest-path distance, and let $f'$ be the flow just afterward. Let $v$ be the vertex with minimum $\delta_{f'} (s, v)$ whose distance was decreased by the augmentation, so that $\delta_{f'}(s, v) < \delta_f (s, v)$. Let $p = s \rightsquigarrow u \rightarrow v$ be a shortest path from $s$ to $v$ in $G_{f'}$, so that $(u, v) \in E_{f'}$, and
\begin{center}
    \begin{equation}
        \label{eqn:26.12}
        \delta_{f'} (s, u) = \delta_{f'} (s, v) - 1.
    \end{equation}
\end{center}

Because of how we chose $v$, we know that the distance of vertex $u$ from the source $s$ did not no decrease, i.e.,
\begin{center}
    \begin{equation}
        \label{eqn:26.13}
        \delta_{f'}(s, u) \geq \delta_f (s, u).
    \end{equation}
\end{center}
We claim that $(u, v) \notin E$. Why? If we had $(u, v) \in E_f$, then we would also have
\begin{center}
    \begin{align*}
        \delta_f (s, v) &\leq \delta_f (s, u) + 1 && \text{(by lemma 24.10, the triangle inequality)} \\
        &\leq \delta_{f'} (s, u) + 1 \text{(by inequality } (\ref{eqn:26.13}) \text{)} \\
        &= \delta_{f'}(s, v) \text{(by equation } (\ref{eqn:26.12}\text{)}
    \end{align*}
\end{center}
which contradicts our assumption that $\delta_{f'} (s, v) < \delta_f (s, v)$. \\
How can we have $(u, v) \notin E_f$ and $(u, v) \in E_f$? The augmentation must ha ve increased the flow from $v$ to $u$. the Edmonds-Karp algorithm always augments flow along shortest paths, and therefore it augmented along a shortest path from $s$ to $u$ in $G_f$ that has $(v, u)$ as its last edge. Therefore,
\begin{center}
    \begin{align*}
        \delta_f (s, v) &= \delta_f (s, u) - 1 \\
        &\leq \delta_{f'} (s, u) - 1 && \text{(by inequality (\ref{eqn:26.13}))} \\
        &= \delta_{f'} (s, v) - 2 && \text{(by equation (\ref{eqn:26.12}))}
    \end{align*}
\end{center}
which contradicts our assumption that $\delta_{f'}(s, v) < \delta_f (s, v)$. We conclude that our assumption that such a vertex $v$ exists is incorrect.

\subsubsection*{Theorem 26.8}
If the Edmonds-Karp algorithm is run on a flow network $G = (V, E)$ with source $s$ and sink $t$, then the total number of flow augmentations performed by the algorithm is $O(VE)$. \\
\textbf{\textit{Proof}} We say that an edge $(u, v)$ i na residual network $G_f$ is \textit{\textbf{critical}}on an augmenting path $p$ if the residual capacity of $p$ is the residual capacity of $(u, v)$. After we have augmented flow along an augmenting path, anycritical edge on the path disappears from the residual network. Moreover, at least one edge on any augmenting path must be critical. \\
Let $u$ and $v$ be vertices in $V$ that are connected by an edge in $E$. Since augmenting paths are shortest paths, when $(u, v9$ is critical for the first time, we have
$$
\delta_f (s, v) = \delta_f (s, u) + 1.
$$
Once the flow is augmented, the edge $(u, v)$ disappears from the residual network. It cannot reappear later on another augmenting path until after the flow from $u$ to $v$ is decreased, which occurs only if $(v, u)$ appear on an augmenting path. If $f'$ is the flow in $G$ when this event occurs, then we have
$$
\delta_{f'} (s, u) = \delta_{f'} (s, v) + 1.
$$
Since $\delta_f (s, v) \leq \delta_{f'} (s, v)$ by lemma 26.7, we have
\begin{center}
    \begin{align*}
        \delta_{f'} (s, u) &= \delta_{f'} (s, v) + 1 \\
        &\geq \delta_f (s, v) + 1 \\
        &= \delta_f (s, u) + 2.
    \end{align*}
\end{center}
Consequently, from the time $(u, v)$ become critical to the time when it next becomes critical, the distance of $u$ from the source increases by at least $2$- The distance of $u$ from the source is initially at least $0$. The intermediate vertices on a shortest path from $s$ to $u$ cannot contain $s$, $u$ or $t$. Therefore, until $u$ becomes unreachable from the source, if ever, its distance is at most $|V| - 2$. Thus, after the first time that $(u, v)$ becomes critical, it can become critical at most $(|V| - 2)/2 = |V|/2 -1$ times more, for a total of at most $|V|/2$ times. Since there are $O(E)$ pair of vertices that can have anedge between them in a residual network, the total number of critical edges during the entire execution of the Edmonds-Karp algorithm is $O(VE)$. Each augmenting path has at least one critical edge, and hence the theorem follows.

\subsubsection*{Lemma 26.9}
Let $G = (V, E)$ be a bipartite graph with vertex partition $V = L \cup R$, and let $G' = (V', E')$ be its corresponding flow network. If $M$ is amatching in $G$, then there is an integer-valued flow $f$ in $G'$ with value $|f| = |M|$. Conversely, if $f$ is an integer-valued flow in $G'$, then there is a matching $M$ in $G$ with cardinality $|M| = |f|$. \\
\textit{\textbf{Proof}} We first show that a matching $M$ in $G$ corresponds to an integer-valued flow $f$ in $G'$. Define $f$ as follows. If $(u, v) \in M$, then $f(s, u) = f(u, v) = f(v, t) = 1$. For all other edges $(u, v) \in E'$, we define $f(u, v) = 0$. It is simple to verify that $f$ satisfies the capacity constraint and flow conservation. \\
Intuitively, each edge $(u, v) \in M$ corresponds to one uit of flow in $G'$ that traverses the path $s \to u \to v \to t$. Moreover, the paths induced by edges in $M$ are vertex-disjoint, except for $s$ and $t$. The net flow across cut $(L \cup \{s\}, R \cup \{t\})$ is equal to $|M|$; thus the value of the flow is $|f| = |M|$. \\
To prove the converse, let $f$ be an integer-valued flow in $G'$, and let
$$
M = \{(u, v) : U \in L, v \in R, \text{ and } f(u, v) > 0\}.
$$
Each vertex $u \in L$ has only one entering edge and its capacity is $1$. Thus, each $u \in L$ has t most one unit of flow entering it, and if one unit of flow does enter, by flow conservation, one unit of flow must leave. Furthermore, since $f$ is integer-valued, for each $u \in L$, the one unit of flow can enter on at most one edge and can leave on at most one edge. Thus, one unit of flow enters $u$ iff there is exactly one vertex $v \in R$ such that $f(u, v) = 1$, and at most one edge leaving each $u \in L$ carries positive flow. A symmetric argument applies to each $v \in R$. The set $M$ is therefore at matching. \\
To see that $|M| = |f|$, observe that for every matched vertex $u \in L$, we have $f(s, u) = 1$, and for every edge $(u, v) \in E - M$, we have $f(u, v) = 0$. Consequently, $f(L \cup \{s\}, R \cup \{t\})$, the net flow across cut $(L \cup \{s\}, R \cup \{t\})$, is equal to $|M|$. Thus, we have $|f| = f(L \cup \{s\}, R \cup \{t\}) = |M|$.

\clearpage

\section*{Linear Programming and Optimization}
\subsection*{General Knowledge}
\clearpage
\subsection*{Proofs}
\clearpage
\subsection*{Examples}
\clearpage

\section*{Hashing}
\subsection*{General Knowledge}
\clearpage
\subsection*{Proofs}
\clearpage
\subsection*{Examples}
\clearpage

\section*{Van Emde Boas Trees}
\subsection*{General Knowledge}
\clearpage
\subsection*{Proofs}
\clearpage
\subsection*{Examples}
\clearpage

\section*{NP-Completeness}
\subsection*{General Knowledge}
\clearpage
\subsection*{Proofs}
\clearpage
\subsection*{Examples}
\clearpage

\section*{Exact Exponential Algorithms and Parameterized Complexity}
\subsection*{General Knowledge}
\clearpage
\subsection*{Proofs}
\clearpage
\subsection*{Examples}
\clearpage

\section*{Approximation Algorithms}
\subsection*{General Knowledge}
We have at least three waays to get around NP-completeness. First, if the actual inputs are small, an algorithm with exponential running time may be perfectly satisfactory. Second, we may be able to isolate important special cases that we can solve in polynomial time. Third, we might come up with approaches to find \textit{near-optimal} solutions in polynomial time. We call an algorithm that returns near-optimal solutions an \textit{\textbf{approximation algorithm}}
\\
\\
We say that an algorithm for a problem has an \textit{\textbf{approximation ratio}} of $\rho(n)$ if, for any input of size $n$, the cost $C$ of the solution produced by the algorithm is within a factor of $\rho(n)$ of the cost $C^*$ of an optimal solution
\begin{equation}
    \max \left( \frac{C}{C^*}, \frac{C^*}{C} \right) \leq \rho(n).
\end{equation}
If an algorithm achieves an approximation ratio of $\rho(n)$, we call it a \textit{\textbf{$\rho(n)$-approximation algorithm}}. The approximation ratio of an approximation algorithm is never less than 1, since $\frac{C}{C^*} \leq 1$ implies $\frac{C^*}{C} \geq 1$. Therefore, a $1$-approximation algorithm produces an optimal solution.
\\
\\
An \textit{\textbf{approximation scheme}} for an optimization problem is an approximation algorithm that takes as input not opnly an instance of the problem, but also a value $\epsilon > 0$ such that for any fixed $\epsilon$, the scheme is a $(1 + \epsilon)$-approximation algorithm. We say that an approximation scheme is a \textit{\textbf{polynomial-time approximation scheme}} if for any fixed $\epsilon > 0$, the scheme runs in time polynomial in the size $n$ of its input instance.
\\
\\
We say that an approximation scheme is a \textit{\textbf{fully polynomial-time approximation scheme}} if it is an approximation scheme and its running time is polynomial in both $\frac{1}{\epsilon}$ and the size $n$ of the input instance. For example, the scheme might have a running time of $O \left( \left( \frac{1}{\epsilon} \right)^2 n^3 \right)$. With such a scheme, any decrease in $\epsilon$ comes with an instace in the running time.
\\
\\
We say that a randomized algorithm for a problem has an \textit{\textbf{approximation ratio}} of $\rho(n)$ if, for any input of size $n$, the expected cost $C$ of the solution procuded by the randomized algorithm is within a factor of $\rho(n)$ of the cost $C^*$ of an optimal solution:
$$
\max \left( \frac{C}{C^*}, \frac{C^*}{C} \right) \leq \rho(n).
$$
We call a randomized algorithm that achieves an approximation ratio of $\rho(n)$ a \textit{\textbf{randomized $\rho(n)$-approximation algorithm}}

\clearpage

\subsection*{Examples}
\subsubsection*{The Vertex-cover Problem}
\begin{figure}[htbp]
    \centering
    \includegraphics[height = 13cm]{entities/fig_35_1.PNG}
\end{figure}
\noindent A \textit{\textbf{vertex cover}} of an undirected graph $G = (V, E)$ is a subset $V' \subseteq V$ such that if $(u, v)$ is a nedge of $G$, then either $u \in V'$ or $v \in V'$ (or both). The size of a vertex cover is the number of vertices in it. The \textit{\textbf{vertex-cover problem}} is to find a vertex cover of minimum size in a given undirected graph. We call such a vertex cover an \textit{\textbf{optimal vertex cover}}. 
\\
\\
The approximation algorithm \texttt{APPROX-VERTEX-COVER}($G$) returns a vertex cover whose size is guaranteed to be no more than twice the size of an optimal vertex cover. The running time of this algorithm is $O(V + E)$, using adjacency lists to represent $E'$
\begin{algorithm}[htbp]
    \caption{APPROX-VERTEX-COVER}
    \begin{algorithmic}[1]
        \Require Undirected graph $G$
        \State $C = \emptyset$
        \State $E' = G.E$
        \While{$E' \neq \emptyset$}
            \State let $(u, v)$ be an arbitrary edge of $E'$
            \State $C = C \cup \{u, v\}$
            \State remove from $E'$ edge $(u, v)$ and every edge incident on either $u$ or $v$
        \EndWhile
        \State \textbf{return} $C$
    \end{algorithmic}
\end{algorithm}

\subsubsection*{The traveling-salesman problem}
\begin{figure}[htbp]
    \centering
    \includegraphics[height = 13cm]{entities/fig_35_2.PNG}
\end{figure}
In the traveling-salesman problem, we are given a complete undirected graph $G = (V, E)$ that has a nonnegative integer cost $c(u, v)$ associated wit heach edge $(u, v) \in E$, and we must find a hamiltonian cycle of $G$ with minimum cost. As an extension of our notation, let $c(A)$ denote the total cost of the edges in the subset $A \subseteq E$.
\\
\\
We say that the cost function $c$ satisfies the \textit{\textbf{triangle inequality}} if, for all vertices $u, v, w \in V$
$$c(u, w) \leq c(u, v) + c(v, w).$$
\\
\\
We shall first compute a minimum spanning tree, whose weight gives a lower bound on the le ngth of an optimal traveling-salesman tour. We shall then use the minimum spanning tree to create a tour whose cost is no more than twice that of the minimum spanning tree's weight, as long as the cost function satisfies the triangle inequality. \texttt{APPROX-TSP-TOUR}($G$, $c$) implements this approach with a running time of $\Theta(V^2)$ depending on how a simple implementation of how to compute the minimum spanning tree
\begin{algorithm}[htbp]
    \caption{APPROX-TSP-TOUR}
    \begin{algorithmic}[1]
        \Require Complete undirected graph
        \Require Cost function $c$ satisfying the triangle inequality
        \State select a vertex $r \in G.V$ to be a "root" vertex
        \State compute a minimum spanning tree $T$ for $G$ from root $r$
        \State let $H$ be a list of vertices, ordering according to when they are first visited in a preorder tree walk of $T$
        \State \textbf{return} the hamiltionian cycle $H$
    \end{algorithmic}
\end{algorithm}

\subsubsection*{The set-covering problem}
An instance $(X, F)$ of the \textit{\textbf{set-covering problem}} consists of a finite set $X$ and a family $F$ of subsets of $X$, such that every element of $X$ belongs to at least one subset in $F$. We say that a subet $S \in F$ \textit{\textbf{covers}} its elements. The problem is to find a minimum-size subset $C \subseteq F$ whose members cover all of $X$ (both $F$ and $C$ are thus sets of multiple sets). We say, that any $C$ that cover all of $X$, \textit{\textbf{covers}} $X$.
\\
\\
The greedy method works by picking, at each stage, the set $S$ that covers the greatest number of remaining elements that are uncovered
\begin{algorithm}[htbp]
    \caption{GREEDY-SET-COVER}
    \begin{algorithmic}[1]
        \Require A finite set $X$
        \Require A family $F$ of subsets of $X$
        \State $U = X$
        \State $C = \emptyset$
        \While{$U \neq \emptyset$}
            \State Select an $S \in F$ that maximizes $|S \cap U|$
            \State $U = U - S$
            \State $C = C \cup \{S\}$
        \EndWhile
        \State \textbf{return} $C$
    \end{algorithmic}
\end{algorithm}
We can easily implement the algorithm to run in time polynomial in $|X|$ and $|F|$. Since the number of iterations of the loop is bounded from above by $\min \left(|X|, |F| \right)$, and we can implement the loop body to run in time $O(|X||F|)$, a simple implementation runs in time $O(|X||F| \min(|X|, |F|))$.

\subsubsection*{MAX-3-CNF satisfiability}
A particular instance of 3-CNF satisfiability may or may not be satisfiable. In order to be satisfiable, there must exist an assignment of the variables so that every clause evaluates to $1$. If an instance is not satisfiable, we may want to compute how "close" to satisfiable it is, that is, we may wish to find an assignment of the variables that satisfies as many clauses as possible. We call the resulting maximization problem \textbf{\textit{MAX-3-CNF satisfiability}}. The input to MAX-3-CNF satisfiability is the same as for 3-CNF satisfiability, and the goal is to return an assignment of the variables that maximizes the number of clauses evaluating to 1. We require each clause to consist of exactly three distinct literals. We further assume that no clause contains both a variable and its negation.

\subsection*{Vertex cover using linear programming}
In the \textit{\textbf{minimum-weight vertex-cover problem}}, we are given an undirected graph $G = (V, E)$ in which each vertex $v \in V$ has an associated positive weight $w(v)$. for any vertex cover $V' \subseteq V$, we define the weight of the vertex cover $w(V') = \sum_{v \in V'} w(v)$. The goal is to find a vertex cover of minimum weight. We shall compute a lower bound on the weight of the minimum-weight vertex cover, by using a linear program. We shall then "round" this solution and use it to obtain a vertex cover.
\\
\\
Suppose, that we associate a variable $x(v)$ with each vertex $v \in V$, and let us require that $0 \leq x(v) \leq 1$ for each $v \in V$. We put $v$ into the vertex cover iff $x(v) \geq 1/2$. Then, we can write the constraint that for any edge $(u, v)$, at least one of $u$ and $v$ must be in the vertex cover as $x(u) + x(v) \geq 1$. This view gives rise to the \textit{\textbf{linear-programming relaxation}} for finding a minimum-weight vertex cover
\begin{figure}
    \centering
    \includegraphics[width = 9 cm]{entities/LP_relaxation.PNG}
\end{figure}
The procedure \texttt{APPROX-MIN-WEIGHT-VC}($G$, $w$) uses the solution to the linear-programming relaxation to construct an approximate solution to the minimum-weight vertex-cover problem
\begin{algorithm}
    \centering
    \caption{APPROX-MIN-WEIGHT-VC}
    \begin{algorithmic}[1]
        \Require Undirected graph $G = (V, E)$
        \Require Positive weight function $w(v)$ for each $v \in V$
        \State $C = \emptyset$
        \State Compute $\bar{x}$, an optimal solution to the lienar program 
        \For{each $v \in V$}
            \If{$\bar{x}(v) \geq 1/2$}
                $ C = C \cup \{v\}$
            \EndIf
        \EndFor
        \State \textbf{return} C
    \end{algorithmic}
\end{algorithm}

\clearpage

\subsection*{Proofs}
\subsubsection*{Theorem 35.1}
\texttt{APPROX-VERTEX-COVER} is a polynomial-time 2-approximation algorithm
\\
\\
\textit{\textbf{Proof}} It has been shown in a previous chapter, that it runs in polynomial time. \\
The set $C$ of vertices that is returned by the algorithm is a vertex cover, since the laogrithm loops until every edge in $G.E$ has been covered by some vertex in $C$. \\
Let $A$ denote the set of edges that line $4$ picked. Not two edges in $A$ share an endpoint. Thus no two edges in $A$ are covered by the same vertex from an optimal cover $C^*$, and we have the lower bound
\begin{equation}
    \label{eqn:vertex_cover_optimal_lower_bound}
    |C^*| \geq |A|
\end{equation}
on the size of an optimal vertex cover. Since $A$ consists of the edges between two vertices in $C$ (and since all of the elements in $C$ are unique), we have the (exact) upper bound on the size of the vertex cover returned
\begin{equation}
    \label{eqn:vertex_cover_returned_upper_bound}
    |C| = 2|A|
\end{equation}
Combining equation (\ref{eqn:vertex_cover_optimal_lower_bound}) and (\ref{eqn:vertex_cover_returned_upper_bound}), we obtain
$$|C| = 2|A| \leq 2|C^*|$$
\subsection*{Theorem 35.2}
\texttt{APPROX-TSP-TOUR} is a polynomial-time 2-approximation algorithm for the traveling-salesman problem with the triangle inequality
\\
\\
\textit{\textbf{Proof}} It has been shown in a previous chapter, that it runs in polynomial time. \\
Let $H^*$ denote an optimal tour for the given set of vertices. We obtain a spanning tree by deleting any edge fro ma tour, and each edge cost is nonnegative. Therefore, the weight of the minimum spanning tree $T$ provides a lower bound on the cost of an optimal tour
\begin{equation}
    \label{eqn:35.4}
    c(T) \leq c(H^*).
\end{equation}
A \textit{\textbf{full walk}} of $T$ lists the vertices when they are first visited and also whenever they are returned to after a visit to a subtree. Let us call this full walk $W$. Since the full walk traverses every edge of $T$ exactly twice, we have 
\begin{equation}
    \label{eqn:35.5}
    c(W) = 2c(T)
\end{equation}
Inequality (\ref{eqn:35.4}) and equation (\ref{eqn:35.5}) imply that
\begin{equation}
    \label{eqn:35.6}
    c(W) \leq 2c(H^*)
\end{equation}
and so the cost of $W$ is within a factor of 2 of the cost of an optimal tour. \\
Unfortunately, the full walk $W$ is generally not a tour, since it visits some vertices more than once. By the triangle inequality, however, we can delete a visit to any vertex from $W$ and the cost does not increase (If we delete a vertex $v$ from $W$ between visits to $u$ and $w$, the resulting ordering specifies going directly from $u$ to $w$). By repeatedly applying this operation, we can remove from $W$ all but the first visit to each vertex. This ordering is the same as that obtained by a preorder walk of the tree $T$. Let $H$ be the cycle corresponding to this preorder walk. It is a hamiltonian cycle, since every vertex is visited exacly once, and in fact it is the cycle computed by the algorithm. Since $H$ is obtained by deleting vertices from the full walk $W$, we have
\begin{equation}
    \label{eqn:35.7}
    c(H) \leq c(W).
\end{equation}
Combining inequalities (\ref{eqn:35.6}) and (\ref{eqn:35.7}) gives $c(H) \leq 2c(H^*)$, which completes the proof.

\subsubsection*{Theorem 35.4}
\texttt{GREEDY-SET-COVER} is a polynomial-time $\rho(n)$-approximation algorithm, where
$$\rho(n) = H(\max\{|S| : S \in F\})$$
and $H(d) = \sum_{i = 1} ^d 1/i$ is the $d$th harmonic number.
\\
\\
\textit{\textbf{Proof}} It has been shown in a previous chapter, that it runs in polynomial time. \\
To show that the algorithm is a $\rho(n)$-approximation algorithm, we assign a cost of $1$ to each set selected by the algorithm, distribute this cost over the elements covered for the first time, and then use these costs to derive the desired relationship between the size of an optimal set cover $C^*$ and the size of the set cover $C$ returned by the algorithm. Let $S_i$ denote the $i$th subset selected by the algorithm. The algorithm incurs a cost of $1$ when it adds $S_i$ to $C$. We spread this cost of selection $S_i$ evenly among the elements covered for the first time by $S_i$. Let $c_x$ denote the cost allocated to element $x$, for each $x \in X$. Each element is assigned a cost only once, when it is covered for the first time. If $x$ is covered for the first time by $S_i$, then
$$c_x = \frac{1}{|S_i - (S_i \cup S_2 \cup ... \cup S_{i - 1}|}.$$
Each step of the algorithm assigns $1$ unit of cost, and so
\begin{equation}
    \label{eqn:35.9}
    |C| = \sum_{x \in X} c_x.
\end{equation}
Each element $x \in X$ is in at least one set in the optimal cover $C^*$, and so we have $\sum_{S \in C^*} \sum_{x \in S} c_x \geq \sum_{x \in X} c_x$. Combining equation (\ref{35.9}) and inequalitiy (\ref{35.10}), we have that
\begin{equation}
    \label{eqn:35.11}
    |C| \leq \sum_{s \in C^*} \sum_{x \in S} c_x.
\end{equation}
The remainder of the proof rests on the following key inequality, which we shall prove shortly. For any set $S$ belonging to the family $F$
\begin{equation}
    \label{eqn:35.12}
    \sum_{x \in S} c_x \leq H(|S|).
\end{equation}
From inequalities (\ref{35.11}) and (\ref{35.12}) it follows that
$$|C| \leq \sum_{S \in C^*} H(|S|) \leq |C^*| \cdot H(\max\{|S| : S \in F\}),$$
thus proving the theorem. \\
All that remains is to pro ve inequality (\ref{eqn:35.12}). Consider any set $S \in F$ and any $i = 1, 2, ..., |C|$, and let $u_i =|S - (S_1 \cup S_2 \cup ... \cup S_i)$ be the number of elements in $S$ that remain uncovered after the algorithm has selected sets $S_1, S_2, ..., S_i$. We define $u_0 = |S|$ to be the number of elements of $S$, which are all initially uncovered. Let $K$ be the least index such that $u_k = 0$, so that every element in $S$ is uncovered by at least one of the sets $S_1, S_2, ..., S_k$ and some element in $S$ is uncovered by $S_1 \cup S_2 \cup ... \cup S_{k - 1}$. Then, $u_{i - 1} \geq u_i$, and $u_{i - 1} - u_i$ elements of $S$ are covered for the first time by $S_i$, for $i = 1, 2, ..., k$. Thus,
$$
\sum_{x \in S} c_x = \sum_{i = 1} ^k (u_{i - 1} - u_i) \cdot \frac{1}{|S_i - (S_i \cup S_2 \cup ... \cup S_{i - 1})|}.
$$
Observe that
$$
|S_i - (S_1 \cup S_2 \cup ... \cup S_{i - 1}) \geq |S - (S_1 \cup S_2 \cup ... \cup S_{i - 1}| = u_{i - 1},
$$
because the greedy choice of $S_i$ guarantees that $S$ cannot cover more new elements than $S_i$ does. Consequently, we obtain
$$
\sum_{x \in S} c_x \leq \sum_{i = 1} ^k (u_{i - 1} - u_i) \cdot \frac{1}{u_{i - 1}}.
$$
We now bound this quantitiy as follows
$$
\sum_{x \in S} c_x \leq \sum_{i = 1} ^k (u_{i - 1} - u_i) \frac{1}{u_{i - 1}}
$$
$$= \sum_{i = 1} ^k \sum_{j = u_i + 1} ^{u_i - 1} \frac{1}{u_{u_i - 1}}$$
$$\leq \sum_{i = 1} ^k \sum_{j = u_i + 1} ^{u_i - 1} \frac{1}{j} \quad \text{(because } j \leq u_{u_i - 1} \text{ )}$$
$$= \sum_{i = 1} ^k (H(u_{i - 1}) - H(u_i))$$
$$= H(u_0) - H(u_k) \quad \text{(because the sum telescopes)}$$
$$= H(u_0) - H(0)$$
$$= H(u_0) \quad \text{(because } H(0) = 0 \text{)}$$
$$= H(|S|)$$
which completes the proof of inequalities (\ref{eqn:35.12}).

\subsubsection*{Corollary 35.5}
\texttt{GREEDY-SET-COVER} is a polynomial-time $(\ln |X| + 1)$-approximation algorithm.
\\
\\
\textit{\textbf{Proof}} Use inequality (A.14) and Theorem Theorem 35.4

\subsubsection*{Theorem 35.6}
Given an instance of MAX-3-CNF satisfiability with $n$ variables $x_1, x_2, ..., x_n$ and $m$ clauses, the randomized algorithm that independently sets each variable to 1 with probability $1/2$ and to 0 with probability $1/2$ is a randomized $8/7$-approximation algorithm.
\\
\\
\textit{\textbf{proof}} Suppose that we have independently set each variable to $1$ with probability $1/2$ and to 0 with probability $1/2$. For $i = 1, 2, ..., m$, we define the indicator random variable
$$Y_i = I\{\text{clause } i \text{ is satisfied}\}$$
so that $Y_i = 1$ as long as we have set at least one of the literals in the $i$th clause to $1$. Since no literal appears more than once in the same clause, and since we have assumed that no variable and its negation appear in the smae clause, the settings of the three literals in each clause are independent. A clause is not satisfied only if all three of its literals are set to $0$, and so $Pr\{\text{clause } i \text{ is not satisfied}\} = (1/2)^3 = 1/8$. Thus, we have $Pr\{\text{clause } i \text{ is satisfied}\} = 1 - 1/8 = 7/8$, and $E[Y_i] = 7/8$. Let $Y$ be the number of satisfied clauses overall, so that $Y= Y_1 + Y_2 + ... + Y_m$. Then, we have
$$E[Y] = E \left[ \sum_{i = 1} ^m Y_i \right] = \sum_{i = 1} ^m E[Y_i] = \sum_{i=1} ^m 7/8 = 7m/8$$.
Clearly, $m$ is an upper bound on the number of satisfied clauses, and thence the approximation ratio is at most $m/(7m/8) = 8/7$.

\subsubsection*{Theorem 35.7}
Algorithm \texttt{APPROX-MIN-WEIGHT-VC} is a polynomial-time 2-approximation algorithm for the minimum-weight vertex-cover problem.
\\
\\
\textit{\textbf{Proof}} Because there is a polynomial-time algorithm to solve the lienar program in line 2, and because the for-loop runs in polynomial time, the algorithm is a polynomial-time algorithm. \\
Nowe we show that the algorithm is a 2-approximation algorithm. Let $C^*$ be an optimal solution to the minimum-weight vertex-cover problem, and let $z^*$ be the value of an optimal solution to the linear program. Since an optimal vertex cover is a feasible solution to thel inear program, $z^*$ must be a lower bound on $w(C^*)$, that is,
\begin{equation}
    \label{eqn:35.21}
    z^* \leq w(C^*).
\end{equation}
Next, we claim that by rounding the fractional values of the variables $\bar{x}(v)$, we produce a set $C$ that is a vertex cover and satisfies $w(C) \leq 2z^*$. To see that $C$ is a vertex cover, consider any edge $(, v) \in E$. By the first constraint, we know that $x(u) + x(v) \geq 1$, which implies that at least one of $\bar{x}(u)$ and $\bar{x}(v)$ is at least $1/2$. Therefore, at least one of $u$ and $v$ is included in the vertex cover, and so every edgge is covered. \\
Now, we consider the weight of the cover. We have
$$
z^* = \sum_{v \in V} w(v) \bar{x}(v) \geq \sum_{v \in V: \bar{x} \geq 1/2} w(v) \bar{x}(v) \geq \sum_{v \in V: \bar{x}(v) \geq 1/2} w(v) \cdot \frac{1}{2} = \sum_{v \in C} w(v) \cdot \frac{1}{2} = \frac{1}{2} \sum_{v \in C} w(v) = \frac{1}{2}w(C)
$$
\begin{equation}
    \label{eqn:35.22}
    = \frac{1}{2}w(C).
\end{equation}
Combining inequalities (\ref{eqn:35.21}) and (\ref{eqn:35.22}) gives
$$w(C) \leq 2z^* \leq 2w(C^*)$$
and hence \texttt{APPROX-MIN-WEIGHT-VC} is a 2-approximation algorithm.

\clearpage

\section*{Polygon Triangulation}
\subsection*{General Knowledge}
This topic is about the \textit{Art Gallary problem}; how many caomeras do we need to guard a given gallary and how do we decide to place them? We model a gallary as a polygonal region in the plane. We further restrict ourselves to geions that are \textit{simple polygons} (that is, a polygon of a single chain that does not intersect itself). 
\\
\\
Let $\mathcal{P}$ be a simple polygon with $n$ vertices. Because $\mathcal{P}$ may be a complicated shape, it seems difficult to say anything about the number of cameras we need to guard $\mathcal{P}$. Hence, we first decompose $\mathcal{P}$ into pieces that are easy to guard, namely triangles. We do this by drawing diagonals between pair of vertices.
\\
\\
A decomposition of a polygon into triangles by a maximal set of non-intersecting diagonals is called a \textit{triangulation} of the polygon. We require that the set of non-intersecting diagonals to be maximal to ensure that no triangle has a polygon vertex in the interior of one of its edges. We can thus guard $\mathcal{P}$ by placing a camera in every triangle of a triangulation $T_\mathcal{P}$ of $\mathcal{P}$.
\\
\\
Placing cameras at vertices seems better, because a vertex can be incident to many triangles, and a camera at that vertex guards all of them. This suggest the following approach: let $T_\mathcal{P}$ be a triangulation of $\mathcal{P}$. Select a subset of the vertices of $\mathcal{P}$, such that any triangle in $T_\mathcal{P}$ has at least one selected vertex, and place the cameras at the selected vertices.
\\
\\
To find such a subset we assign each vertex of $\mathcal{P}$ a color: white, grey or black. The coloring will be such that any vertices connected by an edge or a diagonal have different colors. this is called a \textit{3-coloring} of a triangulated polygon. In a 3-coloring of a triangulated polygon, every triangle has a white, black and gray vertex. Hence, if we place cameras at all gray vertics, say, we have guarded the whole polygon. By choosing the smallest color class to place the cameras, we guard $\mathcal{P}$ using at most $\left \lfloor \frac{n}{3} \right \rfloor$ cameras.
\\
\\
Let $P$ be a simple polygon with $n$ vertices. A set of $\left \lfloor \frac{n}{3} \right \rfloor$ camera position in The 3-coloring approach is optimal in worst case such that any point inside The 3-coloring approach is optimal in worst case is visible from at least one of the cameras can be computed in $O(n \log n)$ time.
\\
The previous triangulation algorithm will take quadratic time in worst case. This can be improved for some classes of polygons. For instance, convext polygons: Pick one vertex and draw diagonals from every other vertex, that is nto a neighbor, to the vertex. This runs in linear time. Thus, a possible solution would be to decompose The 3-coloring approach is optimal in worst case into convext pieces and then triangulate the pieces. However, it is difficult to partition a polygon into convex pieces. Therefore, we shall decompose The 3-coloring approach is optimal in worst case into \textit{monotone pieces}, which is a lot easier.
\\
\\
A simple polygon is called \textit{monotone with respect to a line} $L$ if for any line $L'$ perpendicular to $L$, the intersection of the polygon with $L'$ is connected. A polygon that is monotone with respect to the y-axis is called \textit{y-monotone}. The following property is characteristic for y-monotone polygons: if we walk from
a topmost to a bottommost vertex along the left (or the right) boundary chain, then we always move downwards or horizontally, never upwards.
\\
\\
Our strategy to triangulate the polygon $\mathcal{P}$ is to first partition $\mathcal{P}$ into y-monotone pieces, and then triangulate the pieces. We can partition a polygon into monotone pieces as follows. imagine walking from the topmost vertex of $\mathcal{P}$ to the bottommost vertex on either boundary chain. A vertex where the direction in which we walk switches from downward to upward or from upward to downward is called a \textit{turn vertex}. To partition $\mathcal{P}$ into y-monotone pieces, we need to remove the turn vertices, which is done by adding diagonals. If at a turn vertex $v$ both incident edges go down and the interior of the polygon lies above $v$, then we must choose a diagonal that goes up from $v$. $v$ cannot be a turn vertex in either of the two resulting subpolygons. If both incident edges of a turn vertex go up and the interior lies bellow it, we have to choose a diagonal that goes down.
\\
\\
We distinguish five types of vertex in $P$. Four of these types are turn vertices: \textit{Start vertices}, \textit{Split vertices}, \textit{End vertices}, and \textit{merge vertices}. They are defined as follows. A vertex is a start vertex if its two neighbors lie below it and the interior angle at $v$ is less than $\pi$. If the interior angle is greater than $\pi$ then $v$ is a split vertex. A vertex is an end vertex if its two neighbors lie above it and the interior angle at $v$ is less than $\pi$. If the interior angle is greater than $\pi$ then $v$ is a merge vertex. The vertices that are not turn vertices are \textit{regular vertices}. Thus a regular vertex has one of its neighbors above it and the other neighbor below it.
\\
\\
\textit{Lemma 3.4} implies, that $\mathcal{P}$ has been partitioned into y-monotone pieces once we get rid of its split and merge vertices. We do this by adding a diagonal going upward from each split vertex and downward from each merge vertex. Once done, $\mathcal{P}$ has been split into y-monotone pieces.
\\
\\
Let's see how we can add the diagonals for the split vertices. Let $v_1, v_2, ..., v_n$ be a counterclock enumeration of the vertces of $\mathcal{P}$. Let $e_1, ..., e_n$ be the set of edges of $\mathcal{P}$ , where $e_i = \overline{v_i v_{i+1}}$ for $1 \leq i < n$ and $e_n = \overline{v_n v_1}$.
\\
\\
The algorithm moves a line $L$ downward over the plane. The line halts at certain event points; in our case the vertices of $\mathcal{P}$. The event points are stored in an event queue $Q$. The event queue is a priority queue, where the priority of a vertex is its y-coordinate. This way the next event to be handled can be found in $O(\log n)$ time. The goal of the sweep is to add diagonals from each split vertex to a vertex lying above it. Let $e_j$ be the dge immediately to the left of $v_i$ on the sweep line, and let $e_k$ be the edge immediately to the right of $v_i$ on the sweep line. Then we can always conenct $v_i$ to the lowest vertex in between $e_j$ and $e_k$, and above $v_i$. If there is no such vertex then we can connect $v_i$ to the upper endpoint of $e_j$ or to the upper endpoint of $_k$. This vertex is called the \textit{helper} of $e_j$ and is denoted by $helper(e_j)$. Formally, $helper(ej)$ is defined as the lowest vertex above the sweepline such that the horizontal segment connecting the vertex to $e_j$ lies inside $\mathcal{P}$
\\
\\
Nowe we know how to get rid of split vertices. What about merge vertices. Suppose the sweep line reaches a merge vertex $v_i$. let $e_j$ and $e_k$ be the edge simmediately to the right and left of $v_i$ on the sweepline. Observe that $v_i$ becomes the new helper of $e_j$ when we reach it. We would like to connect $v_i$ to the highest vertex below the sweep line in between $e_j$ and $e_k$. We do not know the highest vertex below the sweep line when we reach $v_i$. But it is easy to find later on: when we reacj a vertex $v_m$, that replaces $v_i$ as the hlper of $e_j$, then this is the vertex we are looking for. So whenever we replace the hlper of some edge, we check whether the old helper is amerge vertex and, if so, we add the diagonal between the old helper and the new one. This diagonal is always added when the new helper is a split vertex, to get rid of the split vertex. It can also happen that the hlper of $e_j$ is not replaced anymore below $v_i$. In this case we can connect $v_i$ to the lower endpoint of $e_j$.
\\
\\
In the above approach, we need to ifnd the dge to the left of each vertex. Therefore we store the edges of $\mathcal{P}$  intersecting the sweep line in the leaves of a dynamic binary search tree $T$. The left-to-right order of the leaves of $T$ corresponds to the left-to-right order of the edges. Because we are only interested in edges to the left of split and merge vertices we only need to store edges in $T$ that have the interior of $P$ to the right. With each edge in $T$ store its helper. The tree $T$ and the helpers stored with the edges form the status of the sweep line algorithms.
\\
\\
The algorithm partitions $\mathcal{P}$ into subpolygons that have to be processed in a later stage. To have easy access to these subpolygons we shall store the subdivision induced by $\mathcal{P}$  and the added diagonals in a doubly-connected edge-list $D$. We assume $P$ is initially specified as a doubly-connected edge-list. The diagonals outputed for the split and merge vertices are added to the doubly-connected edge-list. To access the doubly-connected edge-list we use cross-pointer between the edges in the status structure and the corresponding edges in the doubly-connected edge-list. Adding a diagonal can then be done in constant time.
\\
\\
Running time: Constructing $Q$ takes linear time and initializing $T$ takes constant time. To handle an event during the sweep we perform one operation on $Q$, at most one query, one insertion, and one deletion on $T$, and we insert at most two diagonals into $D$. Priority queues and balanced search trees allow for queries and updates in $O(\log n)$ time, and an insert into $D$ takes $O(1)$ time. Hence, handling and event takes $O(\log n)$ time, and the algorithm runs in $O(n \log n)$ time.
\\
\\
In the following we show, that monotone polygons can be triangulated in linear time. Together with the partition into monotone pieces in $O(n \log n)$ time, this implies that any simple polygon can be triangulated in $O(n \log n)$ time.
\\
\\
Let $\mathcal{P}$ be an y-monotone polygon with $n$ vertices. We assume that $\mathcal{P}$  is \textit{strictly y-monotone}, that is, it does not contain horizontal edges. 
\\
\\
This property is what makes triangulation a monotone polygon easy: we can work our way through $\mathcal{P}$  from top to the bottom on both chains, adding diagonals whenever this is possible.
\\
\\
The triangulation algorithm handles the vertices in order of decreasing y-coordinate. The algorithm requires a stack $S$ as auxilary data structure. Initially the stack is empty; later it contains the vertices of $P$ that have been encountered but may still need more diagonals. When we handle a vertex we add as many diagonals from this vertex to vertices on the stack as possible. These diagonals split off triangles from $\mathcal{P}$. The vertices that have been handled but not split off - the vertices on the stack - are on the boundary of the part of $\mathcal{P}$  that still needs to be triangulated. The lower the vertex (that is, the more recently it has been encountered), the higher on the stack it is. The part of $\mathcal{P}$  that still needs to be triangulated has a particular shape: it looks like an upside down funnel. One boundary of the funnel consists of a part of a single edge of $\mathcal{P}$ , and the other boundary consists of reflex vertices (interior angle is atleast $180^o$). Only the highest vertex (which is at the bottom of the stack) is convex. This property remains true after, we have handled the next vertex, hence, it is an invariant of the algorithm.
\\
\\
Ket's see which diagonals we can add when we handle the next vertex. We distinguish two cases: $v_j$, the next vertex to handle, lies on the same chain as the reflex vertices on the stack, or it lies on the opposite chain
\begin{enumerate}
    \item If $v_j$ lies on the opposite chain, it must be the funnel. We can add diagonals from $v_j$ to all vertices correctly on the stack, except for the last one; this is the upper vertex, so it is already connected to $v_j$. All the vertices are popped from the stack $v_j$ and the vertex previously on top of the stack are pushed onto the stack
    \item The other case is when $v_j$ is on the same chain as the reflex vertices on the stack. First, pop one vertex from the stack; this vertex is already connected to $v_j$ by an edge of $\mathcal{P}$. Next, pop vertex from the stack and connect them to $v_j$ untill we encounter one where this is not possible. When we find a vertex where we cannot connect $v_j$, we push the last vertex that has been popped back onto the stack. After this has been done we push $v_j$ onto the stack
\end{enumerate}
What is the running time of this algorithm? Linear - see pseudocode.

\clearpage
\subsection*{Proofs}
\subsubsection*{Theorem 3.1}
Every simple polygon admits a triangulation and any triangulation of a simple polygon with $n$ vertices consists of exactly $n-2$ triangles.
\\
\\
\textbf{\textit{Proof}} \\
We prove this theorem by inducting on $n$. When $n = 3$ the polygon itself is a triangle and the theorem is trivially true. Let $n > 3$ and assume that the theorem is true for all $m < n$. Let $\mathcal{P}$ be a polygon with $n$ vertices
\\
\\
We first prove the existence of a diagonal in $\mathcal{P}$. Let $v$ be the leftmost vertex of $\mathcal{P}$. Let $u$ and $w$ be the two neighboring vertices of $v$ on the boundary of $\mathcal{P}$. If the open segment $\overline{uw}$ lies in the interior of $\mathcal{P}$, we have found a diagonal. Otherwise, there are one or more vertices inside the triangle defined by $u, v$ and $w$, or on the diagonal $\overline{uw}$. Of those vertices, let $v'$ be the one furthest from the line through $u$ and $w$. The segment connecting $v'$ and $v$ cannot intersect an edge of $\mathcal{P}$, because such an edge would have an endpoint inside the triangle that is farther from the line through $u$ and $w$, contradicting the definition of $v'$. Hence, $\overline{vv'}$ is a diagonal.
\\
\\
Any diagonal cuts $\mathcal{P}$ into two simple subpolygons $\mathcal{P}_1$ and $\mathcal{P}_2$. Let $m_1$ be the number of vertices in $\mathcal{P}_1$ and $m_2$ be the number of vertices in $\mathcal{P}_2$. both $m_1$ and $m_2$ must be smaller than $n$, so by induction $\mathcal{P}_1$ and $\mathcal{P}_2$ can be triangulated. Hence, $\mathcal{P}$ can be triangulated as well.
\\
\\
It remains to prove that any triangulation of $\mathcal{P}$ consists of $n-2$ triangles. Consider an arbitrary diagonal in some triangulation $T_\mathcal{P}$. This diagonal cuts $\mathcal{P}$ into two subpolygons with $m_1$ and $m_2$ vertices, respectively. Every vertex of $\mathcal{P}$ occurs in exactly one of the two subpolygons, except the vertices defining the diagonal, which occurs in both. Hence, $m_1 + m_2 = n + 2$. By induction, any triangulation of $\mathcal{P}_i$ consists of $m_i - 2$ triangles, which implie sthat $\mathcal{P}$ consists of $(m_1 - 2) + (m_2 - 2) = n - 2$ triangles.

\subsubsection*{Theorem 3.2}
The 3-coloring approach is optimal in worst case.
\\
\\
\textbf{\textit{Proof}}
A 3-coloring always exists. To see this, we look at the \textit{dual graph} of $T_\mathcal{P}$. This graph $g(T_\mathcal{P})$ has a node for every triangle $T_\mathcal{P}$. We denote the triangle corresponding to a node $v$ by $t(v)$. There is an arc between two nodes $v$ and $\mu$ if $t(v)$ and $t(\mu)$ share a diagonal. the arcs in $g(T_\mathcal{P})$ correspond to diagonals in $T_\mathcal{P}$. Because any diagonal cuts $\mathcal{P}$ into two, the removal of an edge from $g(T_\mathcal{P})$ splits the graph into two. Hence, $g(T_\mathcal{P})$ is a tree. This means that we can find a 3-coloring using a simple graph traversal, such as depth first search.
\\
\\
While we do the depth first search, we maintain the following invariant
\begin{enumerate}
    \item All vertices of the already encountered trinagles have been colored white, black or gray
    \item No two connected vertices have received the same color.
\end{enumerate}
This implies that we have computed a valid 3-coloring when all triangles have been encountered. 
\\
\\
The depth first search can be started from any node of $g(T_\mathcal{P})$; the three vertices of the corresponding triangle are colored white, gray and black. Now suppose, we reach a node $v$ in $g$, coming from a node $\mu$. Hence $t(v)$ and $t(\mu)$ share a diagonal. Since the vertices of $t(\mu)$ have already been colored, only one vertex remains to be colored. Because $g(T_\mathcal{P})$ is a tree, the other nodes adjacent to $v$ have not been visited yet, and we still have the freedom to give the vertex the remaining color.
\\
\\
We conclude that the 3-coloring approach is optimal in worst case.
\\
\\
\subsubsection*{Lemma 3.4}
A polygon is y-monotone if its has no split vertices or merge vertices.
\\
\\
\textit{\textbf{Proof}} Suppose $\mathcal{P}$ is not y-monotone. We have to prove that $\mathcal{P}$ contains a split or a merge vertex. Since $\mathcal{P}$ is not monotone, there is a horizontal line $\mathcal{P}$ that intersects $\mathcal{P}$ in more than  one connected component. We can choose $L$ such that the leftmost component is a segment and not a single point. Let $p$ be the left endpoint of this segment, and let $q$ be the right endpoint. 
\\
\\
Starting at $q$, we follow the boundary of $\mathcal{P}$ such that $p$ lies to the left of the boundary (this means that we go up from $q$). At some point $r$, we will go up again. If $r \neq p$, then the highest vertex we encountered while going from $q$ to $r$ must be a split vertex. If $r = p$, we again follow the boundary of $\mathcal{P}$ starting at $q$, but in the other direction. 
\\
\\
Let $r'$ be the point where the boundary intersects $L$. We cannot have $r' = p$, because that would mean that the boundary of $\mathcal{P}$ intersects $L$ only twice, contradicting that $L$ intersects $\mathcal{P}$ in more than one component. So we have $r' \neq p$, implying that the lowest vertex we encountered going from $q$ to $r'$ must be a merge vertex.

\subsubsection*{Lemma 3.5}
The algorithm for transforming a polygon monotone adds a set of non-intersecting diagonals that partitions $\mathcal{P}$ into monotone subpolygons.
\\
\\
\textit{\textbf{Proof}} NOT DONE.


\clearpage

\end{document}